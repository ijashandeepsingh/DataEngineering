{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Author: Jashandeep Singh \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing \n",
    "\n",
    "\n",
    "We have 3 input files provided: \"dirty_data.csv\", \"missing_data.csv\". \"warehouses.csv\".\n",
    "\n",
    "The task requires us to perform graphical and/or non-graphical EDA methods to understand\n",
    "the data first and then find and fix the data problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Libraries required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impoting Libraries\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "#system\n",
    "import sys\n",
    "\n",
    "#dataframe and array libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "#scipy \n",
    "from scipy.stats import norm\n",
    "\n",
    "#haversine\n",
    "#!pip install haversine\n",
    "\n",
    "from haversine import haversine, Unit\n",
    "\n",
    "#sklearn \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "\n",
    "#seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "#nltk Sentiment Analyser \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as salz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given that all the datasets are in the same directory as this jupyter notebook. \n",
    "\n",
    "dirty_data = pd.read_csv(\"dirty_data.csv\")\n",
    "\n",
    "missing_data = pd.read_csv(\"missing_data.csv\")\n",
    "\n",
    "warehouse_data = pd.read_csv(\"warehouses.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by observing the \"missing_data\" and \"warehouse_data\" df to observe the column names and row values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Dataset \n",
    "\n",
    "\n",
    "### Step 1: Identifying the missing values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total information of records. \n",
    "missing_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploratory data analysis (EDA)\n",
    "\n",
    "# using seaborn to visualise the missing values in missing_data\n",
    "plt.figure(figsize = (16,8)) \n",
    "sns.heatmap(missing_data.isnull(),yticklabels=False,cbar=True,cmap='cividis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed from the info data and the visualisation that the columns having the missing data in the \"missing_data\" dataframe are: \n",
    "\n",
    "\"nearest_warehouse\"\n",
    "\n",
    "\"order_price\"\n",
    "\n",
    "\"customer_lat\"\n",
    "\n",
    "\"customer_long\" \n",
    "\n",
    "\"order_total\"\n",
    "\n",
    "\"season\"\n",
    "\n",
    "\"distance_to_nearest_warehouse\"\n",
    "\n",
    "\"is_happy_customer\" \n",
    "\n",
    "It is also obsereved that all of these columns have 10 row inputs missing, as the total number of records is 500, and the above mentioned columns have a total count of 490."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Fixing the missing data for columns: \n",
    "\n",
    "#### A.  \"is_happy_customer\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the Polaratiy score for the available data to further compare and fulfill the missing data \n",
    "\n",
    "# Initialising nltk SentimentIntensityAnalyser \n",
    "sentiment = salz()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first approach is to use the nltk sentiment analyzer to find the polarity score for the \"latest_customer_review\" column. Having a positive or a negative valence will help us determine the customer using a net positive or a net negative review that can in short help us find the missing \"is_happy_customer\" column values (True or False for Positve or Negative valence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying the outliers from the polarity scores in the column \"latest_customer_review\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a temp dataset with indexed values of rows with null values \n",
    "happy_cust_missing = missing_data[missing_data.is_happy_customer.isnull()]\n",
    "\n",
    "#singling out the index value list for computing the values into the missing_data df\n",
    "#index values are the same as of the missing_data df\n",
    "index_list = list(happy_cust_missing.index)\n",
    "\n",
    "#initialising variable for the index list loop\n",
    "#the polarity score computation for \"column\" goes in the same sequence of the index list values.\n",
    "\n",
    "n = 0\n",
    "\n",
    "if n != len(index_list):\n",
    "    for column in happy_cust_missing['latest_customer_review' ]:\n",
    "        \n",
    "        #NLTK sentiment document reference:  https://www.nltk.org/howto/sentiment.html\n",
    "        #a review is considered positive if the compound score is > 0.05\n",
    "        \n",
    "        if sentiment.polarity_scores(column)[\"compound\"] > 0.05: \n",
    "                #showing the review and polarity result\n",
    "                print(\"is_happy_customer is True for the review:\",\"\\n\", column,\"\\n\")\n",
    "                \n",
    "                #adding the missing \"is_happy_customer\" value\n",
    "                missing_data.at[index_list[n], \"is_happy_customer\"] = True\n",
    "                n = n+1\n",
    "\n",
    "\n",
    "        else:\n",
    "                #showing the review and polarity result\n",
    "                print(\"is_happy_customer is False for the review:\",\"\\n\", column, \"\\n\")\n",
    "                \n",
    "                #adding the missing \"is_happy_customer\" value\n",
    "                missing_data.at[index_list[n], \"is_happy_customer\"] = False\n",
    "                n = n+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.  \"nearest_warehouse\" \n",
    "\n",
    "- Given that the radius of the earth is 6378 KM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the function based on haversine formula to find distance between two locations \n",
    "# changed the name to hav_distance for code compatibility. \n",
    "# ref: https://edstem.org/courses/5118/discussion/375168\n",
    "\n",
    "\n",
    "#changed the function name to hav_distance() for better compatibility throughout the code.\n",
    "\n",
    "\n",
    "\n",
    "def hav_distance(origin, destination):\n",
    "    lat1 , lon1 = origin\n",
    "    \n",
    "    lat2, lon2 = destination \n",
    "    \n",
    "    radius = 6378 # radius of earth in kms\n",
    "    \n",
    "    \n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    \n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    \n",
    "    a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1))* math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)\n",
    "\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    \n",
    "    d = radius * c\n",
    "    \n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the warehouse_data to a dictionary for better access of lat and long values, as having a dict with keys as the name of the warehouse and values as the combines lat and long data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curating positonal data lists for the warehouse locations\n",
    "#df to dict ref: https://stackoverflow.com/questions/26716616/convert-a-pandas-dataframe-to-a-dictionary\n",
    "\n",
    "warehouse_loc_dict = warehouse_data.set_index(\"names\").T.to_dict(\"list\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#singling out the df with missing \"nearest_warehouse\" values\n",
    "warehouse_missing = missing_data[missing_data.nearest_warehouse.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed that the other values, such as \"customer_lat\", \"customer_long\", \"distance_to_nearest_warehouse\" are present for the rows where \"nearest_warehouse\" is Null, hence we proceed by computing and comparing the result using haversine formula (hav_distance function) to the values provided in warehouse_data.\n",
    "\n",
    "Computing using the defined function and rounding off to 4 decimal places, the warehouse of whom, the distance is computed closest to the given \"distance_to_nearest_warehouse\", will be added in the missing column data.\n",
    "\n",
    "\n",
    "We iterate a loop to compute a every customer lat and long, on at a time, with the 3 warehouses and curate a temporary dictionary with the calculated distances. For that particular customer the plan of approach for computing \"nearest_warehouse\" is to to chose the minimum distance value obtained while computing for all the warehouses. We use min() function to choose the warehouse with the minimum distance to the customer. \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a list of the index values\n",
    "n_warehouse_missing_index = list(warehouse_missing.index)\n",
    "\n",
    "\n",
    "#Iterating a loop for curating lists of missing warehouse customer positional data \n",
    "n = 0\n",
    "\n",
    "c_lat = []\n",
    "c_long = []\n",
    "\n",
    "if n != len(n_warehouse_missing_index):\n",
    "    \n",
    "    \n",
    "    #all the appending is in the same sequence as the index of the rows \n",
    "    for lat in warehouse_missing[\"customer_lat\"]:\n",
    "        c_lat.append(lat)\n",
    "        \n",
    "        \n",
    "    for long in warehouse_missing[\"customer_long\"]:\n",
    "        c_long.append(long)\n",
    "        \n",
    "    n = n+1\n",
    "\n",
    "    \n",
    "    \n",
    "#iterating a loop for computing and adding the missing \"nearest_warehouse\" values.    \n",
    "n = 0 \n",
    "\n",
    "while n != len(n_warehouse_missing_index):\n",
    "    distance_all = {}\n",
    "    \n",
    "    \n",
    "    for key, values in warehouse_loc_dict.items():\n",
    "        \n",
    "        #rounding off the distance value to 4 decimal places\n",
    "        distance_all[key] = round(hav_distance((c_lat[n], c_long[n]), warehouse_loc_dict[key]),4)\n",
    "        \n",
    "    #minimum value from a dict ref: https://stackoverflow.com/questions/3282823/get-the-key-corresponding-to-the-minimum-value-within-a-dictionary\n",
    "    #using the min distance warehouse for filling \"nearest_warehouse\"\n",
    "    \n",
    "    missing_data.at[n_warehouse_missing_index[n], \"nearest_warehouse\"] = min(distance_all, key = distance_all.get)\n",
    "    \n",
    "    \n",
    "    #printing the index and name of nearest warehouse that is computed.\n",
    "    print(\"For Index:\", n_warehouse_missing_index[n], \", the nearest warehouse is:\", min(distance_all, key = distance_all.get))\n",
    "    \n",
    "    #resetting the dictionary \n",
    "    distance_all.clear()\n",
    "    #iterating the loop ahead\n",
    "    n = n+1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. \"distance_to_nearest_warehouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curating a df with null \"distance_to_nearest_warehouse\" values\n",
    "\n",
    "dist_ware_missing = missing_data[missing_data.distance_to_nearest_warehouse.isnull()]\n",
    "\n",
    "#curating an index list\n",
    "\n",
    "dist_ware_missing_index = list(dist_ware_missing.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We approach to find the missing values by making a separate df with the null values and also making an index list of the made dataframe. The index list remains unique and will act as a pointer for us to make the missing data additions into the original file \"missing_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_ware_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the hav_distance() function, we calculate the \"distance_to_nearest_warehouse\" value. We use warehouse_loc_dict that was created earlier, which is a dictionary with keys as name of the warehouses and values as the lat and long data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0 \n",
    "\n",
    "while n!= len(dist_ware_missing_index): \n",
    "    \n",
    "    #using the \"customer_lat\" and \"customer_long\" values for rows with null \"distance_to_nearest_warehouse\".\n",
    "    #fetching the warehouse loc values from \"warehouse_loc_dict\" defined in part B. \n",
    "    #computing the distance based on the \"nearest_warehouse\" given. \n",
    "    #rounding the distance value to 4 decimal places.\n",
    "    \n",
    "    distance = round(hav_distance([dist_ware_missing.loc[dist_ware_missing_index[n], \"customer_lat\"],  dist_ware_missing.loc[dist_ware_missing_index[n], \"customer_long\"]], \n",
    "                               (warehouse_loc_dict[dist_ware_missing.loc[dist_ware_missing_index[n], \"nearest_warehouse\"]]) ), 4) \n",
    "    \n",
    "    \n",
    "    print(\"the distance for index \",dist_ware_missing_index[n], \"is: \", distance)\n",
    "    missing_data.at[dist_ware_missing_index[n], \"distance_to_nearest_warehouse\"] = distance \n",
    "    print(\"correct value added\")\n",
    "    \n",
    "    n = n+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### D. \"customer_lat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given function to find customer latitude\n",
    "#ref: https://edstem.org/courses/5118/discussion/375168\n",
    "\n",
    "\n",
    "def get_lat(dist, lat1, lon1, lon2):\n",
    "    radius = 6378 #radius of earth in kms \n",
    "    \n",
    "    a = -0.5 * math.sin(math.radians(lat1)) \n",
    "    b = (math.sin(math.radians(lon2-lon1)/2)** 2 - 0.5) * math.cos(math.radians(lat1))\n",
    "    x_squa = math.tan(dist / radius /2) ** 2\n",
    "    phi = math.atan2(b,a) ##radius \n",
    "    \n",
    "    A = math.sqrt(a**2 + b**2)\n",
    "    \n",
    "    \n",
    "    s1 = round((-phi - math.asin(((x_squa/(1-x_squa)) - 0.5)/A) - math.pi) / math.pi * 180 , 7)\n",
    "    \n",
    "    s2 = round((-phi + math.asin(((x_squa/ (1-x_squa)) - 0.5)/A)) / math.pi * 180 , 7)\n",
    "    \n",
    "    return s1, s2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the same approach, we make a seperate df and index list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_lat_missing = missing_data[missing_data.customer_lat.isnull()]\n",
    "\n",
    "cust_lat_missing_index = list(cust_lat_missing.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cust_lat_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we simillarly iterate a loop over the len of the index list and single out customer longitude, warehouse name, warehouse position (based on warehouse name) and distance for each row for the index in the index list. \n",
    "\n",
    "using the get_lat() function provided and mentioned above, we compute the customer latitude value for the specific index. We also see that the function produces 2 results and hence we observed both the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0 \n",
    "\n",
    "\n",
    "while n!= len(cust_lat_missing_index):\n",
    "    \n",
    "    #here x = (cust_lat_missing.at[cust_lat_missing_index[n],\"customer_lat\"])\n",
    "    \n",
    "    customer_long = cust_lat_missing.at[cust_lat_missing_index[n],\"customer_long\"]\n",
    "\n",
    "    ware_name = cust_lat_missing.at[cust_lat_missing_index[n],\"nearest_warehouse\"]\n",
    "\n",
    "    warehouse_pos =  warehouse_loc_dict[str(ware_name)]\n",
    "    \n",
    "    distance = cust_lat_missing.at[cust_lat_missing_index[n], \"distance_to_nearest_warehouse\"]\n",
    "    \n",
    "    \n",
    "    #using the provided get_lat function \n",
    "    \n",
    "    customer_lat = get_lat(distance, warehouse_pos[0], warehouse_pos[1], customer_long)\n",
    "    \n",
    "    \n",
    "    print(\"for index value:\", n, \"customer_lat values are:\", customer_lat)\n",
    "\n",
    "    #using the distance formula to find the best option of the latitudes. \n",
    "    \n",
    "    distance_1 = hav_distance((customer_lat[0], customer_long),(warehouse_pos))\n",
    "    print(\"for customer_lat 1 value , distance1: \", round(distance_1, 4))\n",
    "                              \n",
    "    distance_2 = hav_distance((customer_lat[1], customer_long),(warehouse_pos))\n",
    "    print(\"for customer_lat 2 value distance2: \", round(distance_2, 4))\n",
    "    \n",
    "    #since both the latitude values generate the same rounded distance, we choose one value to update in the df\n",
    "    \n",
    "    # index values as the guide to update \"missing_data\"\n",
    "    missing_data.at[cust_lat_missing_index[n], \"customer_lat\"] = customer_lat[0]\n",
    "    \n",
    "    print(\"for index value \", n , \"customer_lat value added\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    n = n + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly obsereved that both the results are the same so we chose a single latitude point and added in the \"missing_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. \"customer_long\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the  fucntion to find customer longitude \n",
    "# ref: https://edstem.org/courses/5118/discussion/375168\n",
    "\n",
    "\n",
    "\n",
    "def get_lon(dist, lat1, lon1, lat2):\n",
    "    \n",
    "    radius = 6378 #radius of earth in kms \n",
    "    \n",
    "    x_squa = math.tan(dist / radius/2 )** 2\n",
    "    top = math.sqrt((x_squa / (1+ x_squa) - math.sin(math .radians(lat2 - lat1)/2) ** 2)**2)\n",
    "    bot = math.cos(math.radians(lat1)) * math.cos(math.radians(lat2))\n",
    "    \n",
    "    s1 = round(2 * math.asin(math.sqrt(top/bot)) / math.pi * 180 + lon1, 7)\n",
    "    \n",
    "    s2 = round(2 * math.asin(-math.sqrt(top/bot)) / math.pi * 180 + lon1, 7)\n",
    "    \n",
    "    return s1, s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the same approach, curting different df and index list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_long_missing = missing_data[missing_data.customer_long.isnull()]\n",
    "\n",
    "cust_long_missing_index = list(cust_long_missing.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_long_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, similar to part D, we find the customer longitude values by using the provided get_long() funtion. Using the same ideology and approach as part D, we single out all the values (here customer latitude values) and calculate the longitude values. Also, as same, the function produces 2 values, for which, both the distances were same, and hence one values was added in the \"missing_data\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0 \n",
    "\n",
    "\n",
    "while n!= len(cust_long_missing_index):\n",
    "    \n",
    "    \n",
    "    \n",
    "    customer_lat = cust_long_missing.at[cust_long_missing_index[n],\"customer_lat\"]\n",
    "\n",
    "    ware_name = cust_long_missing.at[cust_long_missing_index[n],\"nearest_warehouse\"]\n",
    "\n",
    "    warehouse_pos =  warehouse_loc_dict[str(ware_name)]\n",
    "    \n",
    "    distance = cust_long_missing.at[cust_long_missing_index[n], \"distance_to_nearest_warehouse\"]\n",
    "    \n",
    "    \n",
    "    #using the provided get_long function \n",
    "    \n",
    "    customer_long = get_lon(distance, warehouse_pos[0], warehouse_pos[1], customer_lat)\n",
    "    \n",
    "    \n",
    "    print(\"for index value:\", n, \"customer_long values are:\", customer_long)\n",
    "\n",
    "    #using the distance formula to find the best option of the latitudes. \n",
    "    \n",
    "    distance_1 = hav_distance((customer_lat, customer_long[0]),(warehouse_pos))\n",
    "    print(\"for customer_lat 1 value , distance1: \", round(distance_1, 4))\n",
    "                              \n",
    "    distance_2 = hav_distance((customer_lat, customer_long[0]),(warehouse_pos))\n",
    "    print(\"for customer_lat 2 value distance2: \", round(distance_2, 4))\n",
    "    \n",
    "    #since both the longitude values generate the same rounded distance, we choose one value to update in the df\n",
    "    \n",
    "    \n",
    "    # index values as the guide to update \"missing_data\"\n",
    "    missing_data.at[cust_long_missing_index[n], \"customer_long\"] = customer_long[0]\n",
    "    \n",
    "    print(\"for index value \", n , \"customer_lat value added\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    n = n + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F.  \"season\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the same approach, to make a different df and index list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a df with season null values \n",
    "season_missing = missing_data[missing_data.season.isnull()]\n",
    "\n",
    "#index list for missing season value rows\n",
    "season_missing_index = list(season_missing.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data[\"season\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed that our dataset contains 4 seasons \"Autumn\", \"Summer\", \"Spring\", \"Winter\". Accoring to the classification given, \n",
    "\n",
    "\n",
    "    In Australia, the seasons are defined by grouping the calendar months in the following way:\n",
    "\n",
    "    Spring - the three transition months September, October and November.\n",
    "    Summer - the three hottest months December, January and February.\n",
    "    Autumn - the transition months March, April and May.\n",
    "    Winter - the three coldest months June, July and August.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed by adding a column named \"month\" in \"season_missing\" df, that singles out the month for the date provided in the same row. Our plan of action is to match the month for that row for an integer range set to define the season. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_missing[\"month\"] = pd.DatetimeIndex(season_missing['date']).month\n",
    "\n",
    "#ignore the warning, raised for creating a copy on the slice of original df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function \"season_finder()\" which returns the name of the season based on the month value in the date. The range for the month values for deciding the season is based upon the information given in the assignment instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function that returns season name with month number as input \n",
    "\n",
    "def season_finder(month_number):\n",
    "    \n",
    "    #for months March, April and May\n",
    "    if month_number in range(3,5):\n",
    "        return \"Autumn\"\n",
    "    \n",
    "    #for months June, July and August\n",
    "    if month_number in range(6,8):\n",
    "        return \"Winter\"\n",
    "    \n",
    "    #for months September, October and November\n",
    "    if month_number in range(9,11):\n",
    "        return \"Spring\"\n",
    "    \n",
    "    #for months December, January and February\n",
    "    else:\n",
    "        return \"Summer\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterating a loop limiting over length of index list, for each missing season value in the row, we compute season from the added \"month\" column and add the missing season value in the \"missing_data\" for the same index. \n",
    "\n",
    "Also, since the \"month\" column is added in our speratly created df, our \"missing_data\" df is not affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#initialising variable for while loop \n",
    "n = 0 \n",
    "\n",
    "while n!= len(season_missing_index):\n",
    "    \n",
    "    #using \"season_finder\" funtion fetching the season name for the missing \"season\" value row.\n",
    "    season_name = season_finder(season_missing.at[season_missing_index[n], \"month\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"the season for index \",season_missing_index[n], \"is: \", season_name)\n",
    "    \n",
    "    #adding the missing \"season\" value into the \"missing_data\" df using the same index value. \n",
    "    missing_data.at[season_missing_index[n], \"season\"] = season_name\n",
    "    \n",
    "    print(\"value addded\", \"\\n\")\n",
    "    \n",
    "    n = n + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### G. \"order_total\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the same approach, making a different df and index list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a df with order_total null values\n",
    "order_total_missing = missing_data[missing_data.order_total.isnull()]\n",
    "\n",
    "#index list for missing \"order_total\" value rows\n",
    "order_total_missing_index = list(order_total_missing.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_total_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed that order_total is calculated by using the formula (coupon_discount is given in %): \n",
    "\n",
    "    order_total = order_price - (order_price * (coupon_discount/100)) + delivery_charges\n",
    "    \n",
    "Hence, we proceed by calculating the \"order_total\" for the rows with missing \"order_total\" values and also rounding them to 2 decimal values\n",
    "\n",
    "\n",
    "iterating over a loop limiting on length of index list, we fetch all teh variables required by teh formula and then compute the order total. Also, rounding the result value to 2 decimal points, maintaing unity in our \"missing_data\" df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiialising variable for while loop \n",
    "\n",
    "n =0 \n",
    "\n",
    "while n != len(order_total_missing_index): \n",
    "    \n",
    "    #order_price value for the specific index value row\n",
    "    order_price = order_total_missing.at[order_total_missing_index[n], \"order_price\"]\n",
    "    \n",
    "    #coupon_discount value \n",
    "    coupon_discount = order_total_missing.at[order_total_missing_index[n], \"coupon_discount\"]\n",
    "    \n",
    "    #delivery_charges value \n",
    "    delivery_charges =  order_total_missing.at[order_total_missing_index[n], \"delivery_charges\"]\n",
    "    \n",
    "    \n",
    "    #calculating order_total value \n",
    "    order_total = round((order_price - (order_price * (coupon_discount/100)) + delivery_charges), 2)\n",
    "    \n",
    "    #print value check\n",
    "    print(\"the order_total for index \",order_total_missing_index[n], \"is: \", order_total)\n",
    "    \n",
    "    #adding the missing \"order_total\" value to \"missing_data\" df\n",
    "    missing_data.at[order_total_missing_index[n], \"order_total\"] = order_total\n",
    "    \n",
    "    #printing confirmation\n",
    "    print(\"value added\", \"\\n\")\n",
    "    \n",
    "    \n",
    "    n = n + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H. \"order_price\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using same approach, making different df and index list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df with order_price null values \n",
    "order_price_missing = missing_data[missing_data.order_price.isnull()]\n",
    "\n",
    "#index list for missing \"order_price\" value rows\n",
    "order_price_missing_index = list(order_price_missing.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_price_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to part G., re-arranging the observed formula, we calculate \"order_price\" by the following formula (coupon_discount is given in %): \n",
    "\n",
    "    order_price = (order_total - delivery_charges)/(1 - (coupon_discount/100))\n",
    "    \n",
    "Hence, we proceed to calculate the missing \"order_price\" values by iterating a loop limiting on the length of the index list and fetching all the variables required by the formula and computing the result. Also, rounding the result to 1 decimal place to maintain unity over the df. (Although all the values are integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialising the variable for the loop\n",
    "n =0 \n",
    "\n",
    "while n != len(order_price_missing_index): \n",
    "    \n",
    "    #order_total value for the specific index value row\n",
    "    order_total = order_price_missing.at[order_price_missing_index[n], \"order_total\"]\n",
    "    \n",
    "    #coupon_discount value \n",
    "    coupon_discount = order_price_missing.at[order_price_missing_index[n], \"coupon_discount\"]\n",
    "    \n",
    "    #delivery_charges value \n",
    "    delivery_charges =  order_price_missing.at[order_price_missing_index[n], \"delivery_charges\"]\n",
    "    \n",
    "    \n",
    "    #calculating order_price value \n",
    "    order_price = round((order_total - delivery_charges)/(1 - (coupon_discount/100)), 1)\n",
    "    \n",
    "    #print value check\n",
    "    print(\"the order_price for index \",order_total_missing_index[n], \"is: \", order_price)\n",
    "    \n",
    "    #adding the missing \"order_price\" value to \"missing_data\" df\n",
    "    missing_data.at[order_price_missing_index[n], \"order_price\"] = order_price\n",
    "    \n",
    "    #printing confirmation\n",
    "    print(\"value added\", \"\\n\")\n",
    "    \n",
    "    \n",
    "    n = n + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Output the corrected file\n",
    "\n",
    "- saving as 'missing_data_solution.csv'\n",
    "\n",
    "\n",
    "\n",
    "using the info() function, we check any irregularities in our analysis. It is observed that all the null values are added, the total non-null count is 500. The number of columns are same as the original file. We also cross check if any syntactic errors are made in the df by comparing the values over unique() function. After no errors or missing data is observed, we create the output file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.nearest_warehouse.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.season.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Saving the final df with all the missing data corrections\n",
    "missing_data.to_csv('missing_data_solution.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirty Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Missing Data Dataset, we start by observing the columns and row values of the dirty_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Our Plan of Action will be to modulate different functions that handle the errors in the data and then running all those functions over a loop interating over the index values in the dirty_data file . The function we define will be checking  the possible syntactic, semantic, and coverage errors and will fix them. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Finding the price for every product in the shopping_cart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by obtaining a direct knowledge of the price of products in the \"shopping_cart\" column. The approach to obtain is to make a separte value matrix of the products in the shopping_cart (having products define as the column headers) and the order_price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "set(chain(missing_data.shopping_cart))\n",
    "# obsering the initial element of every value in the set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After observing the shopping cart, it was concluded that there are only the following 10 products in the shopping cart: \n",
    "\n",
    "\"Lucent 330S\",\n",
    "\n",
    "\"iAssist Line\",\n",
    "\n",
    "\"pearTV\",\n",
    "\n",
    "\"Alcon 10\",\n",
    "\n",
    "\"Universe Note\",\n",
    "\n",
    "\"Olivia x460\",\n",
    "\n",
    "\"iStream\",\n",
    "\n",
    "\"Toshika 750\",\n",
    "\n",
    "\"Candle Inferno\",\n",
    "\n",
    "\"Thunder line\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We choose 200 smallest order_price number (choosen in order), the reason being to increase the efficiency of our value_matrix model to make equations that are compared with the products and the ammount of order of products (the number of a product acts as a linear definer for the equation to be then compared to the value_matrix of the order_price)\n",
    "\n",
    "By using the numpy.linalg least square, we can compute linear equations made by these matrices and the computation will bring us the unique price for each product. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the price of every product in the shopping cart using the filled \"missing_data\" \n",
    "# We use \"missing_data\" as we can rely on the values more than the \"dirty_data\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#creating the shopping products df \n",
    "column_names = [\"Lucent 330S\",\"iAssist Line\",\"pearTV\",\"Alcon 10\",\"Universe Note\",\"Olivia x460\",\"iStream\",\"Toshika 750\",\"Candle Inferno\",\"Thunder line\"]\n",
    "\n",
    "#keeing the range 200 for each product in the matrix \n",
    "df = pd.DataFrame(0,index=range(0,200),columns = column_names)\n",
    "\n",
    "#creating an order price value matrix\n",
    "#defining 200 size for better equations computation\n",
    "\n",
    "order_array = missing_data.nsmallest(200,'order_price')\n",
    "m = order_array\n",
    "\n",
    "price = dict()\n",
    "count = 0\n",
    "\n",
    "for index,row in order_array['order_price'].iteritems():\n",
    "    \n",
    "    row1 =  missing_data.loc[index,'shopping_cart']\n",
    "    \n",
    "    for i in eval(row1): \n",
    "        df.loc[count,i[0]] = i[1]\n",
    "    count = count + 1\n",
    "\n",
    "    \n",
    "#creating a value matrix for each product\n",
    "df_product_array = df[[\"Lucent 330S\",\"iAssist Line\",\"pearTV\",\"Alcon 10\",\"Universe Note\",\"Olivia x460\",\"iStream\",\"Toshika 750\",\"Candle Inferno\",\"Thunder line\"]].to_numpy()\n",
    "order_array = order_array['order_price'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "\n",
    "# computing value for each product \n",
    "\n",
    "round(np.linalg.lstsq(df_product_array,order_array, rcond= None)[0][1].tolist()[0])\n",
    "\n",
    "# for k,i in np.linalg.lstsq(df_array,order_array)[0]:\n",
    "for idx,item in enumerate(column_names):\n",
    "    \n",
    "    #price dict having the price for the products in the shopping cart\n",
    "    price[item]=round(np.linalg.lstsq(df_product_array,order_array, rcond=None)[0][idx].tolist()[0])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing the price of products\n",
    "price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. \"date\" and \"season\" value check function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the \"date_season_check()\" function takes date, season and index arguments.\n",
    "\n",
    "We proceed to check the date and season values. The plan of action is as follows: \n",
    "\n",
    "1. Using pd.to_dataframe() function, we transform all date values to the '%Y-%m-%d' format. The date values that doesn't transform are then handled in the funtion. \n",
    "\n",
    "2. Performing a syntactic check for the \"season\" column values. For the irregularities such as upper or lowercase entries, we unifrom them using the capitalize() function.\n",
    "\n",
    "3. By creating a season_dict, we'll compare the month values from the \"date\" to see the season name and then perform a check over the dirty_data and correct the error. \n",
    "\n",
    "4. By handling the ValueError() raised by irregular date format, we change the date format to '%Y-%m-%d' and update in the dirty_data file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correcting date format \n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "#converting using to_datetime such that all the exceptions are handled by the below defined function \n",
    "dirty_data['date'] = pd.to_datetime(dirty_data['date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "#converting the season values to same format \n",
    "dirty_data['season'] = [x.capitalize() for x in list(dirty_data['season'])]\n",
    "\n",
    "def date_season_check(date,season,index):\n",
    "    \n",
    "    #using the information given in the requirements, defining a dict with season and specific months \n",
    "    season_dict={'Summer':[12,1,2],'Autumn':[3,4,5],'Winter':[6,7,8],'Spring':[9,10,11]}\n",
    "    \n",
    "    \n",
    "    # given date format \n",
    "    date_format = \"%Y-%m-%d\"\n",
    "    \n",
    "\n",
    "    season_list = season_dict.get(season)\n",
    "    \n",
    "    try:\n",
    "        date_obj = datetime.datetime.strptime(date, date_format) \n",
    "        date_month = date_obj.month\n",
    "    \n",
    "        # checking and correcting for wrong season \n",
    "        if season_list:\n",
    "            if date_month not in season_list:\n",
    "                correct_season = [a for a, b in season_dict.items() if date_month in b]\n",
    "\n",
    "                dirty_data.loc[index,'season'] = correct_season[0]\n",
    "                \n",
    "                #print to view changes\n",
    "                print(\"season updated for index: \", index, \"is \", correct_season[0])\n",
    "                return True\n",
    "    \n",
    "    except ValueError:\n",
    "        \n",
    "        # correcting the format of the date using exception handling \n",
    "        correct_date=\"\"\n",
    "        \n",
    "        # if date has year in starting, then day and month are swaped to form correct date\n",
    "        \n",
    "        if '2019' in date.split('-')[0]:\n",
    "            correct_date = date.split('-')[0]\n",
    "            correct_date = correct_date + '-' + date.split('-')[2] \n",
    "            correct_date = correct_date + '-' + date.split('-')[1]\n",
    "            \n",
    "        elif '2019' in date.split('/')[0]:\n",
    "            correct_date = date.split('/')[0]\n",
    "            correct_date = correct_date + '-' + date.split('/')[2] \n",
    "            correct_date = correct_date + '-' + date.split('/')[1]\n",
    "\n",
    "\n",
    "        #for format with year value at last \n",
    "        elif '2019' in date.split(\"-\")[2]: \n",
    "            correct_date = date.split('-')[2]\n",
    "            \n",
    "            # choose month according to given season from day and month.\n",
    "            \n",
    "            #for the format with month in the middle \n",
    "            if date.split('-')[1] in season_list:\n",
    "\n",
    "                correct_date = correct_date + '-' + date.split('-')[0]\n",
    "                correct_date = correct_date + '-' + date.split('-')[1] \n",
    "                \n",
    "                \n",
    "            #for the format with day in the middle\n",
    "            else:\n",
    "                correct_date = correct_date + '-' + date.split('-')[1]\n",
    "                correct_date = correct_date + '-' + date.split('-')[0]\n",
    "\n",
    "        date_obj1 = datetime.datetime.strftime(datetime.datetime.strptime(correct_date, date_format),date_format)\n",
    "        \n",
    "        #updating in the dirty_data\n",
    "        dirty_data.at[index,'date'] = date_obj1\n",
    "        \n",
    "        #print to view changes \n",
    "        print(\"date updated for index: \", index, \" \", date_obj1)\n",
    "        \n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. \"customer_lat\", \"customer_long\", \"nearest_warehouse\", \"distance_to_nearest_warehouse\" check funtion\n",
    "\n",
    "the \"warehouse_dict_check()\" takes 5 arguments which are the latitude, longitude, distance_to_nearest_warehouse, nearest_warehouse and index value. We proceed to check and correct the errors as follows:\n",
    "\n",
    "1. Correcting the format of \"nearest_warehouse\" values to the original format using the capitalize() function. \n",
    "\n",
    "2. perfomring a value check to see wether the lat or long values are not 0.\n",
    "\n",
    "3. performing a check to see wether the \"nearest_warehouse\" values are the only ones given in \"warehouse_data\" file. (only 3 names). Also seeing wether the name is not something else. \n",
    "\n",
    "4. performing a check to see the distance values matching to requirement and not 0. if not equal to computed distance, correction is made.\n",
    "\n",
    "5. updating the corrections made in the dirty_data file and also rounding off the required values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the format of the names as required. \n",
    "dirty_data['nearest_warehouse'] = [x.capitalize() for x in list(dirty_data['nearest_warehouse'])]\n",
    "\n",
    "\n",
    "def warehouse_dist_check(lat,long,warehouse_dist,nearest_warehouse,index): \n",
    "    dist = 0.0\n",
    "    columns = list(warehouse_data)\n",
    "    \n",
    "    # swap the lat and long, by checking their values\n",
    "    \n",
    "    #making sure lat and long are not zero \n",
    "    if (lat>0 or long<0): \n",
    "        \n",
    "        dirty_data.loc[index,'customer_lat'] = long\n",
    "        dirty_data.loc[index,'customer_long'] = lat \n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        cust_lat_long = [lat,long]\n",
    "        \n",
    "        #checking if warehouse name not in the provided warehouse_data \n",
    "        if (nearest_warehouse[0] not in 'NTB'):\n",
    "            dirty_data.at[index,'nearest_warehouse'] = nearest_warehouse\n",
    "            return True\n",
    "        \n",
    "        \n",
    "        #warehouse_lat_long =  warehouse_loc_dict[str(nearest_warehouse)]\n",
    "        ware_lat_long = (warehouse_data[warehouse_data['names']==nearest_warehouse]['lat'].astype(float),warehouse_data[warehouse_data['names']==nearest_warehouse]['lon'].astype(float))\n",
    "\n",
    "        #print (\"warehouse locations are:\",warehouse_lat_long, \"\\n\")\n",
    "        #print('customer locations are',cust_lat_long)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for index1,row in warehouse_data.iterrows():\n",
    "            ware_lat_long = list() \n",
    "            ware_lat_long.append(warehouse_data[columns[1]][index1])\n",
    "            ware_lat_long.append(warehouse_data[columns[2]][index1]) \n",
    "            \n",
    "            \n",
    "            if (dist == 0):\n",
    "                \n",
    "                dist = haversine((ware_lat_long),(cust_lat_long))\n",
    "                warehouse = warehouse_data.loc[index1,'names']\n",
    "                \n",
    "            elif (dist > haversine(ware_lat_long,cust_lat_long)):\n",
    "                \n",
    "                dist = haversine(ware_lat_long,cust_lat_long)\n",
    "                \n",
    "                warehouse = warehouse_data.loc[index1,'names']\n",
    "                \n",
    "        if (format(warehouse_dist,'.1f') != format(dist,'.1f')): \n",
    "            dirty_data.loc[index,'distance_to_nearest_warehouse'] = round(dist,4) #rounding to 4 decimal places  \n",
    "            \n",
    "            \n",
    "            #print to view changes \n",
    "            print(\"distance_to_nearest_warehouse updated for index: \", index, \"is \", round(dist,4))\n",
    "            return True\n",
    "            \n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. \"latest_customer_review\" and \"is_happy_customer\" check function\n",
    "\n",
    "\n",
    "the \"happy_customer_check()\" function takes \"customer_review\", \"is_happy_customer\" and index as arguments.\n",
    "\n",
    "the funtion performs the following check and correction:\n",
    "\n",
    "1. seeing over the index value, if the latest_customer_review is actually positive or negative using the nltk sentiment analyser with compound threshold of 0.05. If the review is positive but the \"is_happy_customer\" is False, the correction is made and same logic in the opposite case of review being negative and \"is_happy_customer\" True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def happy_customer_check(customer_review,is_happy_customer,index):\n",
    "    \n",
    "    #using the NLTK sentimentanalyser initialised as 'sentiment'   \n",
    "    #compung threshold of 0.05. above values are positve and below are negative polarity. \n",
    "\n",
    "    if sentiment.polarity_scores(customer_review)[\"compound\"] > 0.05:\n",
    "        \n",
    "        #checking the value \n",
    "\n",
    "        if (is_happy_customer != True): \n",
    "            \n",
    "            #error correction\n",
    "            dirty_data.loc[index,'is_happy_customer'] = True\n",
    "            \n",
    "            #printing to view changes \n",
    "            print(\"is_happy_customer = True  updated for index: \", index)\n",
    "    else:\n",
    "        if (is_happy_customer != False):\n",
    "            dirty_data.loc[index,'is_happy_customer'] = False\n",
    "            \n",
    "            #printing to view changes \n",
    "            print(\"is_happy_customer = False  updated for index: \", index)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \"order_price\", \"delivery_charges\", \"coupon_discount\", \"order_total\" check function \n",
    "\n",
    "the \"order_price_check()\" function performs the following checks and correct the error: \n",
    "\n",
    "1. calculating the \"order_price\" value based over the \"price\" dict (which contains all the individual pricing for each product). \n",
    "\n",
    "2. correcting the \"order_price\" value is the computation results in different value. \n",
    "\n",
    "3. calulating and correcting the \"order_total\" for where errors were present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_price_check(price_item,delivery_charges,coupon_discount,purch_order,index):\n",
    "    \n",
    "    \n",
    "    row1 = dirty_data.loc[index,'shopping_cart'] \n",
    "    #print(row1)\n",
    "    order_price_cal = 0\n",
    "    \n",
    "    \n",
    "    #lower casing the elemets and the names from the price dict for perfect match \n",
    "    product_price_lowercase = {k.lower(): v for k, v in price.items()}\n",
    "        \n",
    "    \n",
    "    for i in eval(row1):\n",
    "    \n",
    "        #using the product pricing \n",
    "        #lowercasing the irregularities in the product name in the \"shopping_cart\"\n",
    "    \n",
    "        order_price_cal = order_price_cal + int(product_price_lowercase.get(i[0].lower()))*int(i[1])\n",
    "\n",
    "    if (price_item != order_price_cal):\n",
    "        order_total_new = order_price_cal - (coupon_discount)*(order_price_cal/100) + delivery_charges\n",
    "        dirty_data.loc[index,'order_total'] = order_total_new \n",
    "        \n",
    "        #printing to view change \n",
    "        print(\"order_total updated for index: \", index, \"is \",order_total_new)\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        #checking for correct calculation of order_price with other depending elements such as cupoun code and delivery charge \n",
    "        \n",
    "        order_price = purch_order + (coupon_discount)*(price_item/100) - delivery_charges\n",
    "        dirty_data.loc[index,'order_price'] = round(order_price)\n",
    "        \n",
    "        #printing to view changes \n",
    "        print(\"order_price updated for index: \", index, \"is \", round(order_price))\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Function to nest all the error correction functions. \n",
    "\n",
    "the \"clean_data()\" function takes the index as the argument and iterates for every column. When the conditions set over column names is satisfied, the function nests the above defined function and performs the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#difining the function to nest all the above defined functions for correcting the dirty_data \n",
    "\n",
    "#using index_1 naming to erradicate any mismatch in nested function computation\n",
    "\n",
    "def clean_data(index_1):\n",
    "    columns = list(dirty_data)\n",
    "    for column in columns:\n",
    "        \n",
    "        #checking for date or season\n",
    "        if \"date\" in column or \"season\" in column:\n",
    "            if (date_season_check(dirty_data.loc[index_1, \"date\"], dirty_data.loc[index_1, \"season\"], index_1)):\n",
    "                continue\n",
    "                \n",
    "        #checking for customer lat, customer long, distance and nearest_warehouse\n",
    "        if \"customer_lat\" in column or \"distance_to_nearest_warehouse\" in column:\n",
    "            if (warehouse_dist_check(dirty_data.loc[index_1, \"customer_lat\"], dirty_data.loc[index_1,\"customer_long\"], dirty_data.loc[index_1,\"distance_to_nearest_warehouse\"], dirty_data.loc[index_1,\"nearest_warehouse\"], index_1)):\n",
    "                continue\n",
    "        \n",
    "        #checking for order price or delivery charges\n",
    "        if \"order_price\" in column or \"delivery_charges\" in column:\n",
    "            if (order_price_check(dirty_data.loc[index_1, \"order_price\"], dirty_data.loc[index_1, \"delivery_charges\"], dirty_data.loc[index_1, \"coupon_discount\"], dirty_data.loc[index_1, \"order_total\"], index_1)):\n",
    "                continue\n",
    "                \n",
    "        if \"is_happy_customer\" in column:\n",
    "            happy_customer_check(dirty_data.loc[index_1, \"latest_customer_review\"], dirty_data.loc[index_1, \"is_happy_customer\"] ,index_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Call for \"clean_data( )\" function to start the cleaning of the \"dirty_data\"\n",
    "\n",
    "- also producing a change log made in the dirty_data with the type, value, index value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function call to clean the dirty data \n",
    "\n",
    "print(\"The Change Log for the dirty_data file is follows: \", \"\\n\")\n",
    "\n",
    "for index, row in dirty_data.iterrows():\n",
    "    clean_data(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output of the corrected \"dirty_data\"\n",
    "\n",
    "- performing a safety check using info() and unique() function to observe the irregularities are cleaned and no new anomolies are added.\n",
    "\n",
    "- ouput the cleaned file with the required naming 'dirty_data_solution.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dirty_data.date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.nearest_warehouse.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.season.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output cleaned file \n",
    "\n",
    "dirty_data.to_csv('dirty_data_solution.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyterthemes\n",
      "  Downloading jupyterthemes-0.20.0-py2.py3-none-any.whl (7.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.0 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jupyter-core in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from jupyterthemes) (4.6.3)\n",
      "Requirement already satisfied: ipython>=5.4.1 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from jupyterthemes) (7.16.1)\n",
      "Collecting lesscpy>=0.11.2\n",
      "  Downloading lesscpy-0.15.0-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 6.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=1.4.3 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from jupyterthemes) (3.2.2)\n",
      "Requirement already satisfied: notebook>=5.6.0 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from jupyterthemes) (6.0.3)\n",
      "Requirement already satisfied: traitlets in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from jupyter-core->jupyterthemes) (4.3.3)\n",
      "Requirement already satisfied: decorator in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from ipython>=5.4.1->jupyterthemes) (4.4.2)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from ipython>=5.4.1->jupyterthemes) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from ipython>=5.4.1->jupyterthemes) (3.0.5)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from ipython>=5.4.1->jupyterthemes) (0.1.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from ipython>=5.4.1->jupyterthemes) (0.17.1)\n",
      "Requirement already satisfied: pygments in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from ipython>=5.4.1->jupyterthemes) (2.6.1)\n",
      "Requirement already satisfied: backcall in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from ipython>=5.4.1->jupyterthemes) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from ipython>=5.4.1->jupyterthemes) (49.2.0.post20200714)\n",
      "Requirement already satisfied: pickleshare in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from ipython>=5.4.1->jupyterthemes) (0.7.5)\n",
      "Requirement already satisfied: six in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from lesscpy>=0.11.2->jupyterthemes) (1.15.0)\n",
      "Requirement already satisfied: ply in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from lesscpy>=0.11.2->jupyterthemes) (3.11)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=1.4.3->jupyterthemes) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.11 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.17.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=1.4.3->jupyterthemes) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=1.4.3->jupyterthemes) (0.10.0)\n",
      "Requirement already satisfied: jinja2 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from notebook>=5.6.0->jupyterthemes) (2.11.2)\n",
      "Requirement already satisfied: nbconvert in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from notebook>=5.6.0->jupyterthemes) (5.6.1)\n",
      "Requirement already satisfied: prometheus-client in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from notebook>=5.6.0->jupyterthemes) (0.8.0)\n",
      "Requirement already satisfied: Send2Trash in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from notebook>=5.6.0->jupyterthemes) (1.5.0)\n",
      "Requirement already satisfied: ipykernel in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from notebook>=5.6.0->jupyterthemes) (5.3.2)\n",
      "Requirement already satisfied: pyzmq>=17 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from notebook>=5.6.0->jupyterthemes) (19.0.1)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from notebook>=5.6.0->jupyterthemes) (0.8.3)\n",
      "Requirement already satisfied: tornado>=5.0 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from notebook>=5.6.0->jupyterthemes) (6.0.4)\n",
      "Requirement already satisfied: nbformat in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from notebook>=5.6.0->jupyterthemes) (5.0.7)\n",
      "Requirement already satisfied: ipython-genutils in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from notebook>=5.6.0->jupyterthemes) (0.2.0)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from notebook>=5.6.0->jupyterthemes) (6.1.6)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.4.1->jupyterthemes) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.4.1->jupyterthemes) (0.2.5)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from jedi>=0.10->ipython>=5.4.1->jupyterthemes) (0.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from jinja2->notebook>=5.6.0->jupyterthemes) (1.1.1)\n",
      "Requirement already satisfied: defusedxml in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.6.0)\n",
      "Requirement already satisfied: bleach in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (3.1.5)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (1.4.2)\n",
      "Requirement already satisfied: testpath in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.4.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.3)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from nbformat->notebook>=5.6.0->jupyterthemes) (3.2.0)\n",
      "Requirement already satisfied: packaging in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (20.4)\n",
      "Requirement already satisfied: webencodings in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/jay/opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (0.16.0)\n",
      "Installing collected packages: lesscpy, jupyterthemes\n",
      "Successfully installed jupyterthemes-0.20.0 lesscpy-0.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jupyterthemes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Themes: \r\n",
      "   chesterish\r\n",
      "   grade3\r\n",
      "   gruvboxd\r\n",
      "   gruvboxl\r\n",
      "   monokai\r\n",
      "   oceans16\r\n",
      "   onedork\r\n",
      "   solarizedd\r\n",
      "   solarizedl\r\n"
     ]
    }
   ],
   "source": [
    "!jt -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jt -t monokai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
